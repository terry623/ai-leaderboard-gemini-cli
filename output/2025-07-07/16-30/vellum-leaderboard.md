# Vellum LLM Leaderboard

Crawled on: 2025-07-07

Data source: https://www.vellum.ai/llm-leaderboard

## Top Models by Task

### Best in Reasoning (GPQA Diamond)

| Rank | Model | Score |
| :--- | :--- | :--- |
| 1 | Gemini 2.5 Pro | 86.4% |
| 2 | Grok 3 [Beta] | 84.6% |
| 3 | OpenAI o3 | 83.3% |

### Best in High School Math (AIME 2025)

| Rank | Model | Score |
| :--- | :--- | :--- |
| 1 | OpenAI o4-mini | 93.4% |
| 2 | Grok 3 [Beta] | 93.3% |
| 3 | Gemini 2.5 Pro | 92% |

### Best in Agentic Coding (SWE Bench)

| Rank | Model | Score |
| :--- | :--- | :--- |
| 1 | Claude 4 Sonnet | 72.7% |
| 2 | Claude 4 Opus | 72.5% |
| 3 | Claude 3.7 Sonnet [R] | 70.3% |

### Best in Tool Use (BFCL)

| Rank | Model | Score |
| :--- | :--- | :--- |
| 1 | Llama 3.1 405b | 81.1% |
| 2 | Llama 3.3 70b | 77.3% |
| 3 | GPT-4o | 72.08% |

### Best in Adaptive Reasoning (GRIND)

| Rank | Model | Score |
| :--- | :--- | :--- |
| 1 | Gemini 2.5 Pro | 82.1% |
| 2 | Claude 4 Sonnet | 75% |
| 3 | Claude 4 Opus | 67.9% |

### Best Overall (Humanity's Last Exam)

| Rank | Model | Score |
| :--- | :--- | :--- |
| 1 | Gemini 2.5 Pro | 21.6% |
| 2 | OpenAI o3 | 20.32% |
| 3 | OpenAI o4-mini | 14.28% |

## Fastest and Most Affordable Models

### Fastest Models (Tokens/second)

| Rank | Model | Speed |
| :--- | :--- | :--- |
| 1 | Llama 4 Scout | 2600 |
| 2 | Llama 3.3 70b | 2500 |
| 3 | Llama 3.1 70b | 2100 |

### Lowest Latency (Seconds to first token)

| Rank | Model | Latency |
| :--- | :--- | :--- |
| 1 | Nova Micro | 0.3s |
| 2 | Llama 3.1 8b | 0.32s |
| 3 | Llama 4 Scout | 0.33s |

### Cheapest Models (USD per 1M tokens)

| Rank | Model | Input Cost | Output Cost |
| :--- | :--- | :--- | :--- |
| 1 | Nova Micro | $0.04 | $0.14 |
| 2 | Gemma 3 27b | $0.07 | $0.07 |
| 3 | Gemini 1.5 Flash | $0.075 | $0.3 |
